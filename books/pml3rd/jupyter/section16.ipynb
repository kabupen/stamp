{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4041beb2-742b-42fc-bffb-1308f064fc1a",
   "metadata": {},
   "source": [
    "# §16.1 系列データ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b31b1-4412-4df7-9741-4df53018b6df",
   "metadata": {},
   "source": [
    "系列データはシーケンス (Sequence) と呼ばれる。\n",
    "シーケンスはこれまでのデータ（例えば画像データ）と異なり、特定の順序で並んでいて、その順序も特徴量の一つであるということである。\n",
    "そこで再帰ニューラルネットワーク（RNN)は、シーケンスをうまく扱うためにモデル化されている。\n",
    "\n",
    "## シーケンスモデルのカテゴリ\n",
    "\n",
    "- many to one\n",
    "    - 入力データはシーケンスで、出力はスカラーであるようなもの。テキストの感情分析などで使用される。\n",
    "- one to many\n",
    "    - 入力データは固定長で、出力がシーケンスであるもの。画像キャプショニングがその一例。\n",
    "- many to many\n",
    "    - 入出力どちらもシーケンスであるもの。機械翻訳がその一例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053816a-e2aa-462b-8471-85e3ac54841d",
   "metadata": {},
   "source": [
    "# §16.2 シーケンスモデルの構築\n",
    "\n",
    "## BOTTを使った学習\n",
    "\n",
    "$$\n",
    "L = \\sum_{t=1}^T L^{t}\n",
    "$$\n",
    "\n",
    "時間 $t$ の損失は、それ以前の全ての時間の隠れユニットに依存するので、勾配は次のように計算できる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-clearance",
   "metadata": {},
   "source": [
    "## SimpleRNN （単層RNN）の実装\n",
    "\n",
    "- SimpleRNN\n",
    "    - units : \n",
    "    - return_sequences : 各時系列ごとの出力を返すか、最後の時刻の出力だけを返すか。ここでは True にしているので、それまでの時系列全ての出力を返す。\n",
    "    \n",
    "- 入力の形状は `(None, None, 5)`\n",
    "    - １つ目：バッチ次元（可変の場合はNone)\n",
    "    - ２つ目：シーケンス（シーケンスが可変長の場合はNone)\n",
    "    - 3つ目：特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "weighted-thought",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "(2, 2)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "rnn_layer = tf.keras.layers.SimpleRNN(units=2, use_bias=True, return_sequences=True)\n",
    "rnn_layer.build(input_shape=(None, None, 5))\n",
    "\n",
    "# 各層の重みを初期化する\n",
    "w_xh, w_oo, b_h = rnn_layer.weights\n",
    "\n",
    "print(w_xh.shape)\n",
    "print(w_oo.shape)\n",
    "print(b_h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-capitol",
   "metadata": {},
   "source": [
    "入力情報を作成する。ここでは $3\\times 5$次元のデータを作成した。行方向に時間軸、列方向に特徴量の個数を取っている。なのでこの練習では、$t=0,1,2$ の時間軸に対してRNNを組んでいる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "invalid-sheffield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力：\n",
      " tf.Tensor(\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3.]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_seq = tf.convert_to_tensor([ [1.0]*5, [2.0]*5, [3.0]*5 ], dtype=tf.float32)\n",
    "print(\"入力：\\n\", x_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-large",
   "metadata": {},
   "source": [
    "作成した時系列データを一つ一つ処理をしていく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "missing-consultation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 ->\n",
      "\t tf.Tensor([1. 1. 1. 1. 1.], shape=(5,), dtype=float32)\n",
      "\t Input \t  [[1. 1. 1. 1. 1.]]\n",
      "\t Hidden \t :  [[0.41464037 0.96012145]]\n",
      "\t output (manual) :  [[0.39240566 0.74433106]]\n",
      "\t SimpleRNN output :  0 [0.39240566 0.74433106] \n",
      "\n",
      "Time step 1 ->\n",
      "\t tf.Tensor([2. 2. 2. 2. 2.], shape=(5,), dtype=float32)\n",
      "\t Input \t  [[2. 2. 2. 2. 2.]]\n",
      "\t Hidden \t :  [[0.82928073 1.9202429 ]]\n",
      "\t output (manual) :  [[0.80116504 0.9912947 ]]\n",
      "\t SimpleRNN output :  1 [0.80116504 0.9912947 ] \n",
      "\n",
      "Time step 2 ->\n",
      "\t tf.Tensor([3. 3. 3. 3. 3.], shape=(5,), dtype=float32)\n",
      "\t Input \t  [[3. 3. 3. 3. 3.]]\n",
      "\t Hidden \t :  [[1.243921  2.8803642]]\n",
      "\t output (manual) :  [[0.95468265 0.9993069 ]]\n",
      "\t SimpleRNN output :  2 [0.95468265 0.9993069 ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = rnn_layer(tf.reshape(x_seq, shape=(1, 3, 5)))\n",
    "\n",
    "out_man = []\n",
    "\n",
    "for t in range(len(x_seq)):\n",
    "\n",
    "    xt = tf.reshape(x_seq[t], (1,5))\n",
    "    print('Time step {} ->'.format(t))\n",
    "    print('\\t', x_seq[t])\n",
    "    print('\\t Input \\t ', xt.numpy())\n",
    "    \n",
    "    ht = tf.matmul(xt, w_xh) + b_h\n",
    "    print(\"\\t Hidden \\t : \", ht.numpy())\n",
    "    \n",
    "    if t > 0 :\n",
    "        prev_o = out_man[t-1]\n",
    "    else :\n",
    "        prev_o = tf.zeros(shape=(ht.shape))\n",
    "        \n",
    "    ot = ht + tf.matmul(prev_o, w_oo)\n",
    "    ot = tf.math.tanh(ot)\n",
    "    \n",
    "    out_man.append(ot)\n",
    "    \n",
    "    print('\\t output (manual) : ', ot.numpy())\n",
    "    print('\\t SimpleRNN output : ', t, output[0][t].numpy(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffba08-68f3-4dbf-a37c-b20f48690f93",
   "metadata": {},
   "source": [
    "## 長期的な相互作用の学習\n",
    "\n",
    "時間方向に深いので、勾配消失もしくは勾配発散の問題が発生する。\n",
    "\n",
    "- 勾配刈り込み\n",
    "- T-BPTT\n",
    "- LSTM\n",
    "\n",
    "\n",
    "## LSTM\n",
    "\n",
    "勾配消失問題を回s欠する方法として提唱された。\n",
    "LSTMの構成要素はメモリセルで、標準的なRNNの隠れ層をこのメモリセルで置き換える。\n",
    "文献を読むときには、LSTMには様々な種類が存在することに留意しておくこと。\n",
    "共通するのは\n",
    "\n",
    "- 忘却ゲート\n",
    "- 入力ゲート\n",
    "- 出力ゲート\n",
    "\n",
    "の3種類のゲートを用いて、記憶を調整するということである。忘却ゲートと入力ゲートでは、その時刻$t$の情報を忘れるか覚えるかを判断し、出力ゲートではこれまでと同様隠れ状態ベクトルを出力する。\n",
    "長期の情報を保持するセルの情報は $C$ として表され、$h$ とはまた別に導入される状態ベクトルであることに留意する。\n",
    "\n",
    "### 忘却ゲート ( $f_t$ )\n",
    "\n",
    "メモリセルを無限に成長させるのではなく、ゲートを通過させる情報と通過させない情報を決定する。\n",
    "\n",
    "$$\n",
    "f_t = \\sigma \\left(\\boldsymbol{W}_{xf}\\boldsymbol{x}^{(t)} + \\boldsymbol{W}_{hf}\\boldsymbol{h}^{(t-1)} + \\boldsymbol{b}_f \\right)\n",
    "$$\n",
    " \n",
    "- $\\sigma$ : シグモイド関数 (活性化関数)\n",
    "- $\\boldsymbol{W}_{xf}$ : 時刻$t$の入力層から忘却ゲートへ結合するときの重み\n",
    "- $\\boldsymbol{W}_{hf}$ : ひとつ前の時刻 $t-1$ から忘却ゲートへ結合するときの重み\n",
    "\n",
    "\n",
    "### 入力ゲート ( $i_t$ ) と候補値 ( $\\tilde{C}_t$ )\n",
    "\n",
    "$$\n",
    "\\boldsymbol{i}_t = \\sigma \\left(\\boldsymbol{W}_{xi}\\boldsymbol{x}^{(t)} + \\boldsymbol{W}_{hi}\\boldsymbol{h}^{(t-1)} + \\boldsymbol{b}_i  \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\tilde{C}}_t = \\mathrm{tanh} \\left(\\boldsymbol{W}_{xc}\\boldsymbol{x}^{(t)} + \\boldsymbol{W}_{hc}\\boldsymbol{h}^{(t-1)} + \\boldsymbol{b}_c  \\right)\n",
    "$$\n",
    "\n",
    "時刻 $t$ のセル状態は次のように計算される\n",
    "\n",
    "$$\n",
    "C^{(t)} = \\left( C^{(t-1)} \\odot f_t \\right) \\oplus (i_t \\odot \\tilde{C}_t)\n",
    "$$\n",
    "\n",
    "日本語で書くと、時刻$t$のセル状態は\n",
    "\n",
    "- ひとつ前の時刻 $t-1$ のセル状態 $\\times$  忘却ゲート (過去のことを記憶するかどうかを判定）\n",
    "- 時刻$t$での入力ゲート $\\times$ 候補値 （現在のことを記憶するかどうかを判定）\n",
    "\n",
    "の重ね合わせで、時刻$t$のセル状態($\\simeq$ 記憶) を更新していく。\n",
    "\n",
    "### 出力ゲート\n",
    "\n",
    "$$\n",
    "o_t = \\sigma \\left(\\boldsymbol{W}_{xo}\\boldsymbol{x}^{(t)} + \\boldsymbol{W}_{ho}\\boldsymbol{h}^{(t-1)} + \\boldsymbol{b}_o  \\right)\n",
    "$$\n",
    "\n",
    "これを用いて、現在時刻の隠れユニットは次のように計算される\n",
    "\n",
    "$$\n",
    "h^{(t)} = o_t \\odot \\mathrm{tanh} \\left( C^{(t)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de5d6d-3b7d-438c-9feb-268c44e2c21c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef56e946-45d8-42b4-aa82-d7c527306b93",
   "metadata": {},
   "source": [
    "# 実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257d225-27a1-4612-91a5-9fb23de60d87",
   "metadata": {},
   "source": [
    "## §16.3.2 \n",
    "\n",
    "テキスト文書を入力して、新しいテキストを生成できるモデルを開発する。入力情報は文字単位に分割され、RNNに一文字ずつ供給される。\n",
    "RNNは受け取った一文字の次の文字を予測するように学習していく。例えば：\n",
    "\n",
    "- データとして Hello world! を与える\n",
    "- 'H', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd', '!' に分割する\n",
    "- 学習としては「H」を受け取ったら「e」、「e」を受け取ったら「l」...という様に次々に学習していく。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16b6a74-fac4-4d69-8b8f-fc851152ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "総文字数 1112350\n",
      "一意な文字種 80\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"1268-0.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "start_index = text.find(\"THE MYSTERIOUS ISLAND\")\n",
    "end_index = text.find(\"End of the Project Gutenberg\")\n",
    "text = text[start_index:end_index]\n",
    "\n",
    "char_set = set(text)\n",
    "\n",
    "print(\"総文字数\", len(text))\n",
    "print(\"一意な文字種\", len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0be5c-8513-42bd-8045-aa28efa8af49",
   "metadata": {},
   "source": [
    "一般的なRNNライブラリでは文字列フォーマットの入力データには対処できない。\n",
    "そのため、このテキストを数値に変換する（マッピングする）必要がある。簡単な辞書を作成し、読み取った文字列をそれに基づいて数値化していく。\n",
    "ここではエンコーディング（文字列を数値化したことをここではエンコードと読んでいる）した結果を `text_encoded` に格納している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9557778c-1a89-4ea7-ae42-16b13b63d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:idx for idx, ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7cc52-7750-4c95-aff0-fc9e530d18c9",
   "metadata": {},
   "source": [
    "作成したエンコード済み文字列から Tensorflowの Datasetを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7582176c-9986-4002-8621-5e7b35950902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 T\n",
      "32 H\n",
      "29 E\n",
      "1  \n",
      "37 M\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
    "\n",
    "for ex in ds_text_encoded.take(5):\n",
    "    print(ex.numpy(), char_array[ex.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aadf660-d80a-4b40-87e3-b46e245f4e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "ds_chunks = ds_text_encoded.batch(chunk_size, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]\n",
    "    target_seq = chunk[1:]\n",
    "    return input_seq, target_seq\n",
    "\n",
    "ds_sequences = ds_chunks.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ecb6cd4-b586-498a-af2e-6394425d23df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  (x) 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
      "Target (y) 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "Input  (x) ' Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n'\n",
      "Target (y) 'Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "for example in ds_sequences.take(2):\n",
    "    print('Input  (x)', repr(''.join(char_array[example[0].numpy()])))\n",
    "    print('Target (y)', repr(''.join(char_array[example[1].numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93d77d97-7cc2-4450-81cd-19df63c70111",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "ds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f6dfe-570c-42e4-a87d-6f7ad935daa6",
   "metadata": {},
   "source": [
    "### ここで keras について\n",
    "\n",
    "- Embedding : \n",
    "    - 最初の入力層としてのみ使用できる\n",
    "    - input_dim, output_dim \n",
    "- LSTM : 長短記憶\n",
    "    - \n",
    "- Dense : 全結合ニューラルネット層\n",
    "    - units (第一引数) : 出力の次元。一般的には `(batch_size, input_dim)` の入力を取り、 `(batch_size, units)` の次元を持つ出力を返す。\n",
    "    - activation = None : 指定しなければ活性化なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a4d73d0-6e41-4b18-883b-488b1d9e4c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 256)         20480     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 512)         1574912   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 80)          41040     \n",
      "=================================================================\n",
      "Total params: 1,636,432\n",
      "Trainable params: 1,636,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "charset_size = len(char_array)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size=charset_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb3e3f-f795-495e-8989-4d3541763006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

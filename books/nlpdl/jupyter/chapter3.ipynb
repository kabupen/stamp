{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nominated-utility",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#順部\" data-toc-modified-id=\"順部-1\">順部</a></span><ul class=\"toc-item\"><li><span><a href=\"#ソフトマックス関数\" data-toc-modified-id=\"ソフトマックス関数-1.1\">ソフトマックス関数</a></span></li><li><span><a href=\"#one-hot-ベクトル\" data-toc-modified-id=\"one-hot-ベクトル-1.2\">one-hot ベクトル</a></span></li></ul></li><li><span><a href=\"#§3.2-言語モデル\" data-toc-modified-id=\"§3.2-言語モデル-2\">§3.2 言語モデル</a></span><ul class=\"toc-item\"><li><span><a href=\"#概要\" data-toc-modified-id=\"概要-2.1\">概要</a></span></li><li><span><a href=\"#確率モデルの定義\" data-toc-modified-id=\"確率モデルの定義-2.2\">確率モデルの定義</a></span></li><li><span><a href=\"#FNN言語モデル\" data-toc-modified-id=\"FNN言語モデル-2.3\">FNN言語モデル</a></span></li></ul></li><li><span><a href=\"#§3.3-分散表現\" data-toc-modified-id=\"§3.3-分散表現-3\">§3.3 分散表現</a></span><ul class=\"toc-item\"><li><span><a href=\"#分散表現\" data-toc-modified-id=\"分散表現-3.1\">分散表現</a></span></li></ul></li><li><span><a href=\"#系列変換モデル\" data-toc-modified-id=\"系列変換モデル-4\">系列変換モデル</a></span><ul class=\"toc-item\"><li><span><a href=\"#モデル構造\" data-toc-modified-id=\"モデル構造-4.1\">モデル構造</a></span><ul class=\"toc-item\"><li><span><a href=\"#符号化器埋め込み層\" data-toc-modified-id=\"符号化器埋め込み層-4.1.1\">符号化器埋め込み層</a></span></li><li><span><a href=\"#符号化器再帰層\" data-toc-modified-id=\"符号化器再帰層-4.1.2\">符号化器再帰層</a></span></li><li><span><a href=\"#復号化器埋め込み層\" data-toc-modified-id=\"復号化器埋め込み層-4.1.3\">復号化器埋め込み層</a></span></li><li><span><a href=\"#復号化器再帰層\" data-toc-modified-id=\"復号化器再帰層-4.1.4\">復号化器再帰層</a></span></li><li><span><a href=\"#復号化器出力層\" data-toc-modified-id=\"復号化器出力層-4.1.5\">復号化器出力層</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-absorption",
   "metadata": {},
   "source": [
    "# 順部"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-finish",
   "metadata": {},
   "source": [
    "## ソフトマックス関数\n",
    "\n",
    "\n",
    "\n",
    "## one-hot ベクトル\n",
    "\n",
    "$x$ が D次元のベクトルで、$\\sum x_d = 1$ を満たすとき（ひとつの要素だけが1）、one-hot ベクトルという。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-chancellor",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "noble-oregon",
   "metadata": {},
   "source": [
    "# §3.2 言語モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-survivor",
   "metadata": {},
   "source": [
    "## 概要\n",
    "\n",
    "- BOS (Beginning of sentence)\n",
    "- EOS (End of sentence)\n",
    "\n",
    "\n",
    "## 確率モデルの定義\n",
    "\n",
    "- $T$個+文頭文末の仮想単語からなる合計 $T+2$ 個の単語からなる文章を $ \\boldsymbol{Y}=( \\boldsymbol{y_0}, \\boldsymbol{y_1},...,\\boldsymbol{y_T}, \\boldsymbol{y_{T+1}})$ とする\n",
    "    - 各単語は one-hot ベクトルで表されている\n",
    "    \n",
    "言語モデルとは文の生成確率 $P(Y)$をモデル化（数式で記述する）することを指す。\n",
    "直前に出現したいくつかの単語を文脈として、次の単語の出現確率をモデル化したものを言語モデルと呼ぶ。\n",
    "\n",
    "- 単語の位置 $t$ より前に出現した $t-a$個の単語を $Y_{[a,t-1]} = (y_a, y_{a+1}, ..., y_{t-1})$ と書く\n",
    "\n",
    "\n",
    "文章 $Y$ が出現する確率は\n",
    "\n",
    "$$\n",
    "P(Y) = P(y_0) \\times \\prod_{t=1}^{T+1} P(y_t|Y_{[0, t-1]})\n",
    "$$\n",
    "\n",
    "で表される。焦点として、$P(y_t|Y_{[0, t-1]})$ をどうモデル化するか？が重要である。\n",
    "\n",
    "## FNN言語モデル\n",
    "\n",
    "前 $C$ 単語だけを入力とし、順伝播型ニューラルネットを用いて $t$ 番目の単語の出現確率をモデル化する。\n",
    "\n",
    "$$\n",
    "P(Y) = \\prod_{t=1}^{T+1} P(y_t | T_{|t-C, t-1|})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-marketing",
   "metadata": {},
   "source": [
    "# §3.3 分散表現\n",
    "\n",
    "自然言語には物理的法則がない（人間が恣意的に決めたもの）ので、類似性や関連性を直接計算することが難しいという問題がある。\n",
    "\n",
    "## 分散表現\n",
    "\n",
    "任意の離散オブジェクト集合 $\\mathcal{V}$ に対して、各離散オブジェクト $v \\in \\mathcal{V}$ にそれぞれD次元ベクトルを割り当てて、離散オブジェクトをD次元ベクトルで表現したもの。\n",
    "\n",
    "離散オブジェクトを $D$ 次元ベクトル空間の写像にする（=埋め込む）。このときのベクトル表現を分散表現と呼ぶ。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 獲得方法\n",
    "\n",
    "離散オブジェクトである各単語にどのような分散表現を割り振るか？= どのようなベクトルとして表現するか？は重要である。\n",
    "\n",
    "### (1) NN を用いる\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-africa",
   "metadata": {},
   "source": [
    "# 系列変換モデル\n",
    "\n",
    "文から文への変換の確率をモデル化したものを系列変換モデル (seq2seq) と呼ぶ。\n",
    "\n",
    "- 機械翻訳\n",
    "- 質問応答\n",
    "\n",
    "ここで離散オブジェクトの列を総称して系列を呼ぶ。自然言語処理では文章を構成する単語の列などが系列に相当する。\n",
    "\n",
    "\n",
    "## モデル構造\n",
    "\n",
    "### 符号化器埋め込み層\n",
    "\n",
    "入力分の各単語をベクトル表現に変換するための層。\n",
    "\n",
    "$$\n",
    "\\bar{x}_i = E^{s}x_i\n",
    "$$\n",
    "\n",
    "ここで $E$ は埋め込み行列。one-hot ベクトル $x_i$ から埋め込みベクトルを取得する方法である。\n",
    "\n",
    "### 符号化器再帰層\n",
    "\n",
    "$$\n",
    "h_i^{(s)} = \\Psi^{(s)} (\\bar{x}_i, h_{i-1}^{(s)})\n",
    "$$\n",
    "\n",
    "一つ前の層と現在の埋め込みベクトルを活性化関数で処理する。\n",
    "よく使用される関数は tanh 関数。\n",
    "\n",
    "\n",
    "\n",
    "### 復号化器埋め込み層\n",
    "\n",
    "### 復号化器再帰層\n",
    "\n",
    "### 復号化器出力層\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-buffer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

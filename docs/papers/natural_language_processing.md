# はじめに（自然言語処理について）

- https://ledge.ai/word2vec/

## 表現

文字列・単語は離散的かつ、物理法則に則った情報ではない。
そのため機械・プログラミングで扱うためには、何らかの数値化を施してやる必要がある。
その数値化された状態を"表現"と呼び、どんな表現を行うかがキーポイントとなっている。


### one-hot ベクトル表現

単語(or一文字)に対して one-hot ベクトルを割り当てていく方法。
扱う単語数が増加するほど、ベクトルの次元数も増加していくので、計算コストが非常に大きくなるという問題があった。
また単語の前後関係など、文章としての情報を取り込めていないことも問題である。

### 分散表現

単語(or一文字）をベクトル表現で表すのは同じであるが、01ではなく連続した実数を割り当ててベクトル空間における向きをもたせた表現方法。
この分散表現により、文字列の四則演算が可能となる。
なぜならベクトルとして扱っているので。混乱してはいけないのはone-hotエンコードもベクトル表現ではあるが、ひとつの要素が1でそれ以外が0という表現であったので互いに直行しており相関を持っていないことに留意せよ（一次独立）。

有名なツールが `Word2Vec`である。
これは単語を分散表現に変換してくれる（埋め込んでくれる）ツールである。

# Inttoduction

- NNでの自然言語処理は一般的になってきている
- 性能は良いが、学習と評価に時間がかかるため、大規模セットでの応用は難しい側面と持つ

- その反面、線形分類機は言語分類にとってベースラインとして考えられている
- シンプルだが良い性能を示す
- 大規模コーパスでの応用可能性を秘めている

本論文では線形分類機のポテンシャルを引き出し、大規模コーパスでの応用を模索した。 rank constraint を用いた線形分類器


# モデル

## BoW (Bag Of Words)

- そもそも文章をベクトル化する（プログラムで扱える形にする）手法は様々考えられる。そのうちの一つが bag of words である
- BoWでは文書中に出現する全てのユニークな単語に番号を割り振り、各単語が何度文書中に出現したかを数え上げてベクトル化する
    - 単純にカウントだけする方法と、単語間の重要度を加味した方法がある
- 文章をベクトル化すれば、あとは高次元空間における分類問題に落とし込むことがきる


センテンス分類にBoWを用いる
線形分類器は特徴とクラスの間でパラメータを共有しない（？）

> linear classifiers do not share parameters among features and classes.

ロジスティック回帰モデル
回帰、とあるが分類モデルであり、線形分離可能なクラスでは非常に性能がよい。
入力データを貰ってロジスティックシグモイド（単にシグモイドとも）関数に入れる。その値を用いてクラスを分類する。

学習は誤差平方和を用いて、入力のデータと真のクラスラベルとの誤差を表現する。誤差平方和を最小化するように内部パラメーターを調整する（最適化は勾配降下法などを用いる）。

$$
\phi(z) = \frac{1}{1-e^z}
$$

Fig1 
入力がN次元


## Hierarchical softmax

- 分類するクラスが多くなると、線形分類の学習のコストが上がってしまう
- 計算コストが $\mathcal{O}(kh)$ になる
    - $k$ : クラス数、$h$ はテキスト表現の次元数
- ランニングタイムを改善するために、階層的ソフトマクスを導入した    
    - コストが下がった
:w

- 階層的ソフトマックスは

Huffman coding-tree に対して階層的ソフトマックスを持ちいた

- $\mathcal{O}(h\log_2(k)$の計算コストに下がった

ハフマン符号化 (Huffman coding-tree)はデータ圧縮技術


## N-gram features

- BoWは登場する文字に番号を振り、その文字が何回登場したかを要素に持つベクトルを作る-->単語の順番を反映していないが、それを取り入れると計算的コストが跳ね上がる
- そこで、bag of n-grams を使用する
    - n-gram を bag にしたもの（番号を振り、登場回数でカウントしてベクトルにする）

### n-gram 
- 文章を文字単位 or 文節単位に区切る
- n個の文字 or 文節をひとまとめにした組み合わせを作る-->n-gram
    - 文字の前後の流れを反映したものが作成できる


# Experiments

fastTextを二種類のタスクで性能評価を行った。
感情分析


## Snetiment analysis

### Results

### Training time


## Tag prediction




# Enriching Word Vectors with Subword Information

## Abstract

- ラベルのない大規模なコーパスで学習した連続的な単語表現は、多くの自然言語処理タスクに有用である。
   - representation : 単語という文字をどう表現するか。単純に one-hot encoding するのかはたまた、、、。どういう表現を導入するか、が一つの鍵
   - 基本的にはベクトル表現に落とし込むので、representaion = vectorization と考えてもよいかも


- 一般的なモデルでは、各単語に個別のベクトルを割り当てることで、単語の形態 (morphology) を無視している
    - こういったモデルを考えるには、個別のベクトルを割り当てるために全ての文字列をまず読み込む必要がある--> 特に語彙数が多く、希少な単語が多い言語においては限界がある

- 本論文では、Skipgramモデルに基づく新しいアプローチを提案する
    - 各単語は character n-gramのバッグとして表現される
    - 各 character n-gramにはベクトル表現が関連付けられており、単語はこれらの表現の合計として表される。
    - n-gram とすることで前後の関係性をもたせることができ、morphology を取り入れたベクトルを作成することができる。

- 本手法は高速であるため、大規模なコーパスを用いたモデルの学習を迅速に行うことができ、
- また、学習データに現れなかった単語の単語表現を計算することができる。我々の単語表現は、9つの異なる言語において、単語の類似性と類推のタスクで評価した。最近提案された形態素解析による単語表現と比較することで、我々のベクトルがこれらのタスクにおいて最先端の性能を達成していることを示す。


## Introduction

- 単語の連続的な表現を学習することは、自然言語処理において長い歴史がある
    - どのように前後関係を数式化するか、が課題

- これらの表現は、通常、共起統計を用いてラベルのない大規模なコーパスから得られるのが一般的
    - co-occurence statistics （共起統計）
    - 単語aと単語bが同時に用いられる回数、のこと（登場する単語同士全てで考えることができ、総当り表のようなものを作成することができる）
    - http://ymatsuo.com/papers/jsai01a.pdf

- 分布意味論 (distributional semantics) は、この共起統計についての研究を行っていた
    - https://www.anlp.jp/proceedings/annual_meeting/2014/pdf_dir/A7-2.pdf

- ニューラルネットワークの分野では、フィードフォワード型のニューラルネットワークを用いて、word embeddingを学習する手法を提案した
    - 単語の前後2単語ずつをもちいて予測を行う
- 最近では、Mikolovら(2013b)が、大規模なコーパス上の単語の連続表現を効率的に学習するために、単純な対数双線形モデルを提案している


- 以上であげた技術のほとんどは、各単語を独立したベクトルとして扱う
    - パラメータの共有がない
    - 特に、単語の内部構造を無視している
- 形態素が豊富な言語に対しては欠点となる
    - 単語が形態変化するので、訓練データセットにそのままの形で現れるのが稀である
    - しかし、ルール（分布）に則って変化しているのでそれを組み入れることで性能は改善される

- 本論文では character n-grams に対する表現の学習方法を提案する


### + 埋め込み (embedding)

- 一意な単語にone-hotエンコーディングを適用する場合、単語数が100000あれば大変なことになる (次元の呪い)
    - またone-hotエンコード特徴量は非オッツを覗いて全て0なので、かなり疎な特徴量

## §3. Model

- morphologyを考慮しながら単語表現を学習するモデルについて議論する
- morphology のモデル化
    - subword units を考慮する
    - character n-grams の集合として単語を表現する


### §3.1 一般的なモデル

- skipgram model（Googleのミコロフが2013年に発表したもの）
    - 本論文で提案されているモデルはここから派生している
- 語彙 V={I, am, like, a, student, apple} と context として I am a studnet を考えてみる
    - 注目する単語(例として am)の前後に登場する単語として I と a を想定する
    - $p(I|am)$ と $p(a|am)$ の確率が大きくなっていると良い <--> p(like|am) の確率は小さくなっていてほしい
    - 以上がskipgramの概要である

- サイズWの語彙を考える
    - 単語にはインデックスが割り振られていて、1〜Wの値を取る
    - 目標は各単語 $w$ のベクトル表現を学習することである
- 大規模な学習用コーパスが $w_1,...w_T$ のシーケンスとして与えられたときに、skipgramモデルの目的は以下の対数尤度関数を最大化することである
    - $\mathcal{Ct}$は $w_t$周りの単語のインデックスである（近接だけでなくどこまで隣接を許すか、をコントロールしている）

$$
\sum_{t=1}^T \sum_{c \in \mathcal{C_t}} \log p(w_c|w_t)
$$

- 単語$w_t$が与えられたときに、$w_c$が観測される確率は、aforementioned word vectors をもちいてパラメとライズされる

- スコア関数 $s$ を考える
    - word と contextの組み合わせをスコアに射影する関数

- ある単語の周りに現れる単語の確率(context word の確率)としてソフトマックス関数を考えることができる

$$
p(w_c|w_t) = \frac{e^s}{\sum e^s}
$$





### §3.2 Subword model

- 単語は character n-gram の bag として表現することができる
- 例として n=3 で where という単語を考える
    - <>を単語に付与して、prefix, suffix も考える
    - <wh, whe, her, er, re> という並びに分けることができる
    - またn=3に限らず、3~6（<where>が7文字なので、6以下のn-gramまでなら考えれる）のn-gram全てを考えることができる

- 
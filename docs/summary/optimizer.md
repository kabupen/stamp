


# 勾配降下法

順伝播型ネットワークの学習（=重み係数の決定）は、誤差関数を $w$ について最小化することである。
誤差関数（損失関数、コスト関数とも）は問題設定によって変わるが、 $E$ で表される。
重み係数が多次元になると想像できないため、3次元空間を擬似的に考える（この例では重みが2個の場合である）。
誤差関数は３次元空間上の平面として表され、重み係数の関数として表面が凹凸を持っているはずである。
目標はこの極小値を求めることである。

- たどり着く極小値は、$w$ の初期値に依存する（ネットワークの性能が初期値を決める乱数に影響される）


勾配降下法 (gradient decet）、確率的勾配降下法（stochastic gradient decent)
